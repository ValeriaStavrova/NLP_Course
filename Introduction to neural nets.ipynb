{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа #6: Introduction to neural nets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе вам предстоит познакомиться с искуственными нейронными сетями. Вообще нейронные сети применимы к широкому спектру задач, однако в области NLP они, как правило, решают задачи классификации. Поэтому сегодня мы будем обучать нейронные сети именно этой задаче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.datasets import load_wine, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим самый базовый датасет для классифицирующих моделей -- $\\href{https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html}{iris}$. Каждая запись в этом датасете предсталяет из себя набор численных характеристик некоторого цветка ириса, целевым параметром является лейбл класса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Число цветков (семлов) = 150\n",
      " Число численных характеристик (фичей) цветка = 4\n",
      " Количество классов = 3\n",
      "\n",
      " Вектора фичей выглядят так:\n",
      "\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.9 3.2 5.7 2.3]]\n",
      " А целевые значения, лейблы классов, выглядят так:\n",
      " [0 0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "n_categories = max(y) + 1\n",
    "print(\" Число цветков (семлов) = {}\\n Число численных характеристик (фичей) цветка = {}\\n Количество классов = {}\\n\".format(X.shape[0], X.shape[1], n_categories))\n",
    "print(\" Вектора фичей выглядят так:\\n\\n {}\".format(X[::30]))\n",
    "print(\" А целевые значения, лейблы классов, выглядят так:\\n {}\".format(y[::30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача классификации состоит в предсказании лейблов класса по данным векторам фичей. Эту задачу можно решать классифицирующими нейронными сетями. Чем мы и займёмся!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем мы приступим к написанию нейронной сети, давайте подготовим данные.\n",
    "\n",
    "Во-первых, хорошей практикой является нормализация данных. Нейронные сети лучше обучаются, если все численные характеристики в векторах численных характеристик располагаются в отрезке [0, 1]. Это можно сделать с помощью MinMaxScaler, который для каждой фичи находит её максимальное и минимальное значение среди всех семплов и относительно них нормализует соответствующий компонент каждого вектора фич.\n",
    "\n",
    "Во-вторых, категориальные характеристики. Целевым значением является лейбл класса, который может принимать только три значения: {0, 1, 2}. Если мы будем хранить это значение как число, то предопредилим нежелательные отношения между классами -- так, класс $1$ будет рассматриваться моделью как более близкий к классу $2$, чем класс $0$, что вовсе не детерминированно условиями задачи или входными данными. Нужно получить такое представление классов, чтоб расстояние между предсталениями двух любых различных классов было одинаково. Это можно сделать с помощью техники $\\href{https://en.wikipedia.org/wiki/One-hot}{One-Hot Encoding}$: давайте кодировать категориальные характеристики с помощью векторов размерности $n = $число различных классов. Каждый такой вектор будет содержать нули во всех своих компонентах кроме той, которая соответствует номеру класса, который мы пытаемся кодировать: в этой компоненте будет стоять единица. Например, последовательность [1, 0, 2, 0...] будет закодирована как [[0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Теперь вектора фичей выглядят так:\n",
      "\n",
      " [[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.13888889 0.45833333 0.10169492 0.04166667]\n",
      " [0.19444444 0.         0.42372881 0.375     ]\n",
      " [0.33333333 0.25       0.57627119 0.45833333]\n",
      " [0.72222222 0.5        0.79661017 0.91666667]]\n",
      " А целевые значения, лейблы классов, выглядят так:\n",
      " [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X, y)\n",
    "y = np.array([np.array([0.0 if i != cat else 1.0 for i in range(n_categories)]) for cat in y])\n",
    "\n",
    "print(\" Теперь вектора фичей выглядят так:\\n\\n {}\".format(X[::30]))\n",
    "print(\" А целевые значения, лейблы классов, выглядят так:\\n {}\".format(y[::30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разделим данные на тренировочные и тестовые. Для нейронных сетей мы будем использовать библиотеку $\\href{https://pytorch.org/}{PyTorch}$, обладающую своими типами и строгой типизацией, так что нам нужно привести данные к её типам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)\n",
    "\n",
    "X_train = Variable(torch.from_numpy(X_train))\n",
    "X_test = Variable(torch.from_numpy(X_test))\n",
    "y_train = Variable(torch.from_numpy(y_train))\n",
    "y_test = Variable(torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте придумаем архитектуру классифицирующей нейронной сети.\n",
    "\n",
    "Число нейронов во входном слое должно соответствовать размерности фича-веткора, в нашем случае это 4. Число нейронов в выходном слое должно соответствовать числу классов, в нашем случае это 3.\n",
    "\n",
    "Очень часто в задачах классификации в качестве функции активации для последнего слоя нейронов используется функция $\\href{https://ru.wikipedia.org/wiki/Softmax}{SoftMax}$. Эта функция преобразовывает вектор таким образом, что каждая его компонента становится неотрицательной, а сумма компонент равняется 1. Из-за этого компоненту под номером i получающегося вектора можно рассматривать как степень уверенности нашей сети в том, что обрабатываемый семпл принадлежит к классу i. Именно таким образом (one-hot) закодированы правильные ответы.\n",
    "\n",
    "Опишем нейронную сеть с одним скрытым слоем при помощи библиотеки $\\href{https://pytorch.org/}{PyTorch}$. Это актуальный и очень серёьзный, но при этом гибкий и удобный инструмент. Смотрите, как просто и интуитивно-понятно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_size = X.shape[1]     #число нейронов входного слоя\n",
    "h_size = 10             #число нейронов скрытого слоя\n",
    "o_size = n_categories   #число нейронов выходного слоя\n",
    "\n",
    "intuitive_model = nn.Sequential(\n",
    "    nn.Linear(i_size, h_size), #линейная операция сложения, числа соответствуют размерностям входного и выходного векторов\n",
    "    nn.ReLU(),                 #функция активации RELU\n",
    "    nn.Linear(h_size, o_size),\n",
    "    nn.Softmax()               #функция активации SoftMax\n",
    ").double()                     #строгая типизация строга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь опишем функцию train_batch, которая принимает пачку примеров, вычисляет усреднённую ошибку нейронной сети на них и соответствующим образом меняет её веса. Это возможно благодаря методу обратного распространения ошибки (gradient descent), который, конечно, помните из лекции. В ходе обратного распространения ошибки необходимо считать градиенты от функции ошибки по векторам весов сети.\n",
    "\n",
    "Хорошие новости: нам не придётся вручную считать градиенты, потому что torch может делать это сам. Нам нужно лишь определить оптимайзер (возьмем стохастический градиентный спуск), функцию ошибки (возьмем бинарную кросс-энтропию) и скорость обучения (learning rate, возьмем 0.05).\n",
    "\n",
    "Когда эти параметры определены, можно вычислять градиенты и менять веса при помощи простых команд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss() #функция ошибки\n",
    "\n",
    "opt= torch.optim.SGD(intuitive_model.parameters(),lr=0.05) #оптимайзер и скорость обучения\n",
    "\n",
    "\n",
    "def train_batch(model, X, y):  # функция, обучающая модель на пачке примеров\n",
    "    y_pred = model(X)          # получим ответ сети для каждого из примеров\n",
    "    ls = loss(y_pred, y)       # вычислим функцию ошибки\n",
    "    opt.zero_grad()            \n",
    "    ls.backward()              # посчитаем градиенты\n",
    "    opt.step()                 # изменяем веса сети\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию train_model, обучающую нейронную сеть в течение n_epochs эпох. Одна эпоха соответствует одному полному просмотру тренировочного датасета. Как уже было отмечено, мы будем обучать сеть пачками (batches) примеров. В рамках одной эпохи мы будем перемешивать тренировочную выборку, делить её на пачки по batch_size примеров, и \"скармливать\" их модели.\n",
    "\n",
    "Давайте обучим нашу сеть на 20 эпохах и оценим результаты её работы на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2\n",
      "\n",
      "Error on train = 0.6340056385838353\n",
      "\n",
      "Error on test = 0.6254987797947752\n",
      "\n",
      "Epoch #4\n",
      "\n",
      "Error on train = 0.6138602905431669\n",
      "\n",
      "Error on test = 0.6078397498651282\n",
      "\n",
      "Epoch #6\n",
      "\n",
      "Error on train = 0.5922749349829853\n",
      "\n",
      "Error on test = 0.5848375838844995\n",
      "\n",
      "Epoch #8\n",
      "\n",
      "Error on train = 0.5645051407073356\n",
      "\n",
      "Error on test = 0.5517177663426328\n",
      "\n",
      "Epoch #10\n",
      "\n",
      "Error on train = 0.5289977395844208\n",
      "\n",
      "Error on test = 0.5137005595381158\n",
      "\n",
      "Epoch #12\n",
      "\n",
      "Error on train = 0.4905232388021281\n",
      "\n",
      "Error on test = 0.47034121849659577\n",
      "\n",
      "Epoch #14\n",
      "\n",
      "Error on train = 0.44594919215898343\n",
      "\n",
      "Error on test = 0.425729419037577\n",
      "\n",
      "Epoch #16\n",
      "\n",
      "Error on train = 0.4088091438986806\n",
      "\n",
      "Error on test = 0.38522310483528566\n",
      "\n",
      "Epoch #18\n",
      "\n",
      "Error on train = 0.37184446204064664\n",
      "\n",
      "Error on test = 0.3510178725562439\n",
      "\n",
      "Epoch #20\n",
      "\n",
      "Error on train = 0.3450953869200572\n",
      "\n",
      "Error on test = 0.3234519065200631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 #рамзер пачки\n",
    "\n",
    "def train_model(model, n_epochs, print_every):\n",
    "    last_test_loss = None\n",
    "    for epoch in range(n_epochs):\n",
    "        n = X_train.shape[0]\n",
    "        random_permutation = torch.randperm(n) #возьмем случайную перестановку примеров\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(0,n,batch_size):\n",
    "            indices = random_permutation[i:i+batch_size] #индексы примеров очередной пачки\n",
    "            batch_x, batch_y = X_train[indices], y_train[indices] # \"скормим\" их модели\n",
    "            total_loss += train_batch(model, batch_x, batch_y) #посчитаем суммарную ошибку на трейне\n",
    "                \n",
    "        if(print_every > 0 and (epoch + 1) % print_every == 0):\n",
    "            pred = intuitive_model(X_test)               #посчитает результат и ошибку на тесте\n",
    "            last_test_loss = loss(pred, y_test)\n",
    "            print(\"Epoch #{}\\n\".format(epoch + 1))\n",
    "            print(\"Error on train = {}\\n\".format(total_loss / (n // batch_size)))\n",
    "            print(\"Error on test = {}\\n\".format(loss(pred, y_test)))\n",
    "            \n",
    "    return last_test_loss\n",
    "    \n",
    "ll = train_model(intuitive_model, 20, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте оценим нашу модель на тестовом сете -- выведем confusion matrix и число неверно классифицированных семплов.\n",
    "\n",
    "Confusion matrix -- матрица размерности n_classes x n_classes, число в i-й строке и j-м столбце соответствует количеству примеров из класса i, которые были классифицированы как j. Сумма главной диагонали, таким образом, соответствует количеству правильных ответов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  0]\n",
      " [ 0  9  7]\n",
      " [ 0  0 16]]\n",
      "Число неправильно классифицированных примеров = 7\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, print_matrix = True):\n",
    "    y_pred = model(X_test) #делаем предсказание на тесте\n",
    "    y_pred_new = torch.argmax(y_pred, dim=1) #ответ сети -- номер компоненты вектора с наибольшим значением. Этот номер соответстует классу.\n",
    "    y_test_true = torch.argmax(y_test, dim=1) #обратно кодируем правильные ответы, чтоб узнать правильные лейблы.\n",
    "    if(print_matrix):\n",
    "        print(confusion_matrix(y_test_true, y_pred_new))\n",
    "    missclass = sum([int(pred != true) for (pred, true) in zip(y_pred_new, y_test_true)]) #вот так можно посчитать число неправильных ответов\n",
    "    return missclass\n",
    "\n",
    "print(\"Число неправильно классифицированных примеров = {}\".format(evaluate_model(intuitive_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, недурно! Но, может быть, можно лучше? Может быть можно выбрать такие параметры сети, что ни один пример не будет классифицирован неправильно?\n",
    "\n",
    "Есть ещё один способ задания сети с помощью PyTorch: кастомизация нейросети путём наследования от базового класса torch.nn.module. Наследующий класс должен реализовать функцию-констукртор __init__, инициализирущую веса сети, и функцию forward, совершающую прямое прохождение по сети. В такой имплементации любые параметры сети (например, количество нейронов в скрытом слое или функция активации на нём) можно сделать параметрами её конструктора.\n",
    "\n",
    "Давайте реализуем класс CustomModel, соответствующий нейронной сети с одним скрытым слоем, число нейронов и функция активации которого будет передаваться на стадии инициализации. Функции train_batch и train_model можно сделать методами нового класса, а все вспомогательные объекты (оптимизатор, learning rate, batch_size) -- его параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=8, activation=torch.nn.ReLU()): #конструктор сети принимает на вход число нейронов скрытого слоя и активирующую его функцию\n",
    "        super(CustomModel,self).__init__()\n",
    "        self.l1 = torch.nn.Linear(i_size, hidden_size)   \n",
    "        self.act=activation                              #конкретная функция activation будет получена как параметр\n",
    "        self.l2 = torch.nn.Linear(hidden_size, o_size)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=0.05)\n",
    "        self.batch_size = 4\n",
    "        \n",
    "    \"\"\"NB: Во всём остальном -- то же самое, что и intuitive_model\"\"\"\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h=self.act(self.l1(x))\n",
    "        return self.softmax(self.l2(h))\n",
    "\n",
    "    def train_batch(self, X, y):\n",
    "        y_pred = self(X)\n",
    "        ls = loss(y_pred, y)\n",
    "        self.opt.zero_grad()\n",
    "        ls.backward()\n",
    "        self.opt.step()\n",
    "        return ls\n",
    "    \n",
    "    def train_model(self, n_epochs, print_every):\n",
    "        last_test_loss = None\n",
    "        for epoch in range(n_epochs):\n",
    "            n = X_train.shape[0]\n",
    "            random_permutation = torch.randperm(n)\n",
    "            total_loss = 0\n",
    "            for i in range(0,n,self.batch_size):\n",
    "                indices = random_permutation[i:i+self.batch_size]\n",
    "                batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "                total_loss += self.train_batch(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте найдем лучшие значения hidden_size и activation, а также оптимальное число обучающих эпох для нашей задачи. Есть множество техник поиска гиперпараметров, самый простой из них -- выбрать несколько значений для разных параметров, перебрать все комбинации и найти лучшее сочетание. Давайте это и сделаем с помощью функции evaluate_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paramset: n_epochs=25, activation_function =ReLU(), hidden_size = 3\n",
      ". Micclassifications = 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "activations = [torch.nn.ReLU(), torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.SELU(), torch.nn.Hardtanh()] #перебираемые функции активации\n",
    "hidden_sizes = [3, 4, 7, 10, 20] #перебираемое число нейронов скрытого слоя\n",
    "epochs = [25, 50] #перебираемое число эпох\n",
    "best_misses = len(y_test) #лучший результат в смысле количества неправильно классифицированных семплов\n",
    "best_paramset = None\n",
    "\n",
    "for paramset in (itertools.product(epochs, activations, hidden_sizes)): #paramset -- tuple, содержащий значения каждого из параметров\n",
    "    e, a, h = paramset\n",
    "    model = CustomModel(hidden_size = h, activation = a).double() #создаём нейронную сеть с перечисленными параметрами\n",
    "    model.train_model(e, 0) #тренируем её в течение %e% эпох \n",
    "    misses = evaluate_model(model, print_matrix = False) #количеств неправильно классифицируемых примеров\n",
    "    if(best_misses > misses): #если результат лучше -- обновляем\n",
    "        best_misses = misses\n",
    "        best_paramset = paramset\n",
    "        \n",
    "print(\"Best paramset: n_epochs={}, activation_function ={}, hidden_size = {}\\n. Micclassifications = {}.\\n\".format(best_paramset[0],best_paramset[1],best_paramset[2],best_misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание: а теперь ваша очередь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваше домашнее задание состоит в том, чтоб по аналогии построить классифицирующую модель для датасета wine. Вам необходимо построить модель, предсказывающую класс вина по его численным характеристикам. Если точнее, от вас требуется:\n",
    "\n",
    "1. Подготовить данные: произвести нормализацию численных данных и one-hot векторизацию категориальных, разделить выборку на тренировочную и тестовую\n",
    "2. Построить и обучить классифицирующую модель на основе нейронной сети, содержащую один скрытый слой. Возьмите такое количество эпох обучения, которое достататочно для того, чтоб функция ошибки перестала падать. Все остальные параметры сети -- на ваше усмотрение.\n",
    "3. Выберите несколько значений параметров hidden_size (число нейронов скрытого слоя), batch_size (число примеров в пачке) и learning_rate(скорость обучения). Опираясь на функцию evaluate_model, найдите лучшую комбинацию параметров. Выведите confusion matrix на тесте для нейронной сети с оптимальными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Число проб вина (семлов) = 178\n",
      " Число численных характеристик (фичей) каждой пробы вина = 13\n",
      " Количество классов = 3\n",
      "\n",
      " Вектора фичей выглядят так:\n",
      "\n",
      " [[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.373e+01 1.500e+00 2.700e+00 2.250e+01 1.010e+02 3.000e+00 3.250e+00\n",
      "  2.900e-01 2.380e+00 5.700e+00 1.190e+00 2.710e+00 1.285e+03]\n",
      " [1.233e+01 1.100e+00 2.280e+00 1.600e+01 1.010e+02 2.050e+00 1.090e+00\n",
      "  6.300e-01 4.100e-01 3.270e+00 1.250e+00 1.670e+00 6.800e+02]\n",
      " [1.208e+01 1.830e+00 2.320e+00 1.850e+01 8.100e+01 1.600e+00 1.500e+00\n",
      "  5.200e-01 1.640e+00 2.400e+00 1.080e+00 2.270e+00 4.800e+02]\n",
      " [1.145e+01 2.400e+00 2.420e+00 2.000e+01 9.600e+01 2.900e+00 2.790e+00\n",
      "  3.200e-01 1.830e+00 3.250e+00 8.000e-01 3.390e+00 6.250e+02]\n",
      " [1.350e+01 3.120e+00 2.620e+00 2.400e+01 1.230e+02 1.400e+00 1.570e+00\n",
      "  2.200e-01 1.250e+00 8.600e+00 5.900e-01 1.300e+00 5.000e+02]]\n",
      " А целевые значения, лейблы классов, выглядят так:\n",
      " [0 0 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_wine(return_X_y=True)\n",
    "n_categories = max(y) + 1\n",
    "print(\" Число проб вина (семлов) = {}\\n Число численных характеристик (фичей) каждой пробы вина = {}\\n Количество классов = {}\\n\".format(X.shape[0], X.shape[1], n_categories))\n",
    "print(\" Вектора фичей выглядят так:\\n\\n {}\".format(X[::30]))\n",
    "print(\" А целевые значения, лейблы классов, выглядят так:\\n {}\".format(y[::30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Теперь вектора фичей выглядят так:\n",
      "\n",
      " [[0.84210526 0.1916996  0.57219251 0.25773196 0.61956522 0.62758621\n",
      "  0.57383966 0.28301887 0.59305994 0.37201365 0.45528455 0.97069597\n",
      "  0.56134094]\n",
      " [0.71052632 0.15019763 0.71657754 0.61340206 0.33695652 0.69655172\n",
      "  0.61392405 0.30188679 0.6214511  0.37713311 0.57723577 0.52747253\n",
      "  0.71825963]\n",
      " [0.34210526 0.07114625 0.49197861 0.27835052 0.33695652 0.36896552\n",
      "  0.15822785 0.94339623 0.         0.16979522 0.62601626 0.14652015\n",
      "  0.28673324]\n",
      " [0.27631579 0.21541502 0.51336898 0.40721649 0.11956522 0.2137931\n",
      "  0.24472574 0.73584906 0.38801262 0.09556314 0.48780488 0.36630037\n",
      "  0.14407989]\n",
      " [0.11052632 0.32806324 0.56684492 0.48453608 0.2826087  0.66206897\n",
      "  0.51687764 0.35849057 0.44794953 0.16808874 0.2601626  0.77655678\n",
      "  0.24750357]\n",
      " [0.65       0.47035573 0.67379679 0.69072165 0.57608696 0.14482759\n",
      "  0.25949367 0.16981132 0.26498423 0.62457338 0.08943089 0.01098901\n",
      "  0.15834522]]\n",
      " А целевые значения, лейблы классов, выглядят так:\n",
      " [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#подготовка данных \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X, y)\n",
    "y = np.array([np.array([0.0 if i != cat else 1.0 for i in range(n_categories)]) for cat in y])\n",
    "\n",
    "print(\" Теперь вектора фичей выглядят так:\\n\\n {}\".format(X[::30]))\n",
    "print(\" А целевые значения, лейблы классов, выглядят так:\\n {}\".format(y[::30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#делим данные на тренировочные и тестовые\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "X_train = Variable(torch.from_numpy(X_train))\n",
    "X_test = Variable(torch.from_numpy(X_test))\n",
    "y_train = Variable(torch.from_numpy(y_train))\n",
    "y_test = Variable(torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_size = X.shape[1]     #число нейронов входного слоя\n",
    "h_size = 10             #число нейронов скрытого слоя\n",
    "o_size = n_categories   #число нейронов выходного слоя\n",
    "\n",
    "intuitive_model = nn.Sequential(\n",
    "    nn.Linear(i_size, h_size), #линейная операция сложения, числа соответствуют размерностям входного и выходного векторов\n",
    "    nn.ReLU(),                 #функция активации RELU\n",
    "    nn.Linear(h_size, o_size),\n",
    "    nn.Softmax()               #функция активации SoftMax\n",
    ").double()                     #строгая типизация строга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss() #функция ошибки\n",
    "\n",
    "opt= torch.optim.SGD(intuitive_model.parameters(),lr=0.05) #оптимайзер и скорость обучения\n",
    "\n",
    "\n",
    "def train_batch(model, X, y):  # функция, обучающая модель на пачке примеров\n",
    "    y_pred = model(X)          # получим ответ сети для каждого из примеров\n",
    "    ls = loss(y_pred, y)       # вычислим функцию ошибки\n",
    "    opt.zero_grad()            \n",
    "    ls.backward()              # посчитаем градиенты\n",
    "    opt.step()                 # изменяем веса сети\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2\n",
      "\n",
      "Error on train = 0.12992322952297533\n",
      "\n",
      "Error on test = 0.14906214437211374\n",
      "\n",
      "Epoch #4\n",
      "\n",
      "Error on train = 0.12170179224649669\n",
      "\n",
      "Error on test = 0.13840590718428017\n",
      "\n",
      "Epoch #6\n",
      "\n",
      "Error on train = 0.11312053925402861\n",
      "\n",
      "Error on test = 0.13270262678998135\n",
      "\n",
      "Epoch #8\n",
      "\n",
      "Error on train = 0.10431674352296103\n",
      "\n",
      "Error on test = 0.12484329441674476\n",
      "\n",
      "Epoch #10\n",
      "\n",
      "Error on train = 0.09719313669063753\n",
      "\n",
      "Error on test = 0.11710684134239979\n",
      "\n",
      "Epoch #12\n",
      "\n",
      "Error on train = 0.09158046327859472\n",
      "\n",
      "Error on test = 0.11340184460061267\n",
      "\n",
      "Epoch #14\n",
      "\n",
      "Error on train = 0.08557490228090629\n",
      "\n",
      "Error on test = 0.10940115619624392\n",
      "\n",
      "Epoch #16\n",
      "\n",
      "Error on train = 0.07922971866537111\n",
      "\n",
      "Error on test = 0.10490513972100735\n",
      "\n",
      "Epoch #18\n",
      "\n",
      "Error on train = 0.07574425414112614\n",
      "\n",
      "Error on test = 0.09834017625896377\n",
      "\n",
      "Epoch #20\n",
      "\n",
      "Error on train = 0.07102091500817372\n",
      "\n",
      "Error on test = 0.09486616189389671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 #рамзер пачки\n",
    "\n",
    "def train_model(model, n_epochs, print_every):\n",
    "    last_test_loss = None\n",
    "    for epoch in range(n_epochs):\n",
    "        n = X_train.shape[0]\n",
    "        random_permutation = torch.randperm(n) #возьмем случайную перестановку примеров\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(0,n,batch_size):\n",
    "            indices = random_permutation[i:i+batch_size] #индексы примеров очередной пачки\n",
    "            batch_x, batch_y = X_train[indices], y_train[indices] # \"скормим\" их модели\n",
    "            total_loss += train_batch(model, batch_x, batch_y) #посчитаем суммарную ошибку на трейне\n",
    "                \n",
    "        if(print_every > 0 and (epoch + 1) % print_every == 0):\n",
    "            pred = intuitive_model(X_test)               #посчитает результат и ошибку на тесте\n",
    "            last_test_loss = loss(pred, y_test)\n",
    "            print(\"Epoch #{}\\n\".format(epoch + 1))\n",
    "            print(\"Error on train = {}\\n\".format(total_loss / (n // batch_size)))\n",
    "            print(\"Error on test = {}\\n\".format(loss(pred, y_test)))\n",
    "            \n",
    "    return last_test_loss\n",
    "    \n",
    "ll = train_model(intuitive_model, 20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 24  1]\n",
      " [ 0  0 15]]\n",
      "Число неправильно классифицированных примеров = 1\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, print_matrix = True):\n",
    "    y_pred = model(X_test) #делаем предсказание на тесте\n",
    "    y_pred_new = torch.argmax(y_pred, dim=1) #ответ сети -- номер компоненты вектора с наибольшим значением. Этот номер соответстует классу.\n",
    "    y_test_true = torch.argmax(y_test, dim=1) #обратно кодируем правильные ответы, чтоб узнать правильные лейблы.\n",
    "    if(print_matrix):\n",
    "        print(confusion_matrix(y_test_true, y_pred_new))\n",
    "    missclass = sum([int(pred != true) for (pred, true) in zip(y_pred_new, y_test_true)]) #вот так можно посчитать число неправильных ответов\n",
    "    return missclass\n",
    "\n",
    "print(\"Число неправильно классифицированных примеров = {}\".format(evaluate_model(intuitive_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выберите несколько значений параметров hidden_size (число нейронов скрытого слоя), \n",
    "#batch_size (число примеров в пачке) и learning_rate(скорость обучения)\n",
    "\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=8, activation=torch.nn.Hardtanh(min_val=-1.0, max_val=1.0),\n",
    "                batch_size = 4, lr = 0.5): #конструктор сети принимает на вход число нейронов скрытого слоя и активирующую его функцию\n",
    "        super(CustomModel,self).__init__()\n",
    "        self.l1 = torch.nn.Linear(i_size, hidden_size)   \n",
    "        self.act=activation                              #конкретная функция activation будет получена как параметр\n",
    "        self.l2 = torch.nn.Linear(hidden_size, o_size)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \"\"\"NB: Во всём остальном -- то же самое, что и intuitive_model\"\"\"\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h=self.act(self.l1(x))\n",
    "        return self.softmax(self.l2(h))\n",
    "\n",
    "    def train_batch(self, X, y):\n",
    "        y_pred = self(X)\n",
    "        ls = loss(y_pred, y)\n",
    "        self.opt.zero_grad()\n",
    "        ls.backward()\n",
    "        self.opt.step()\n",
    "        return ls\n",
    "    \n",
    "    def train_model(self, n_epochs, print_every):\n",
    "        last_test_loss = None\n",
    "        for epoch in range(n_epochs):\n",
    "            n = X_train.shape[0]\n",
    "            random_permutation = torch.randperm(n)\n",
    "            total_loss = 0\n",
    "            for i in range(0,n,self.batch_size):\n",
    "                indices = random_permutation[i:i+self.batch_size]\n",
    "                batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "                total_loss += self.train_batch(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paramset: epoch = 5, hidden_size=3, batch_size =4,learning rate = 0.05\n",
      ". Micclassifications = 0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "epochs = [1, 3, 5, 10, 25, 50]\n",
    "hidden_sizes = [1, 3, 4, 7, 10, 20]\n",
    "batch_sizes = [2, 3, 4, 7, 10]\n",
    "learning_rates = [0.001, 0.01, 0.05, 1]\n",
    "\n",
    "best_misses = len(y_test) \n",
    "best_paramset = None\n",
    "\n",
    "for paramset in (itertools.product(epochs, hidden_sizes, batch_sizes, learning_rates)):\n",
    "    e, h, b, l = paramset\n",
    "    model = CustomModel(hidden_size = h, batch_size=b, lr=l).double() #создаём нейронную сеть с перечисленными параметрами\n",
    "    model.train_model(e, 0) #тренируем её в течение %e% эпох \n",
    "    misses = evaluate_model(model, print_matrix = False) #количеств неправильно классифицируемых примеров\n",
    "    if(best_misses > misses): #если результат лучше -- обновляем\n",
    "        best_misses = misses\n",
    "        best_paramset = paramset\n",
    "print('Best paramset: epoch = {}, hidden_size={}, batch_size ={},'\n",
    "      'learning rate = {}\\n. Micclassifications = {}.\\n'\n",
    "      .format(best_paramset[0],best_paramset[1],best_paramset[2], best_paramset[3], best_misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 25  0]\n",
      " [ 0  0 15]]\n",
      "Число неправильно классифицированных примеров = 0\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel(hidden_size=3, batch_size=4, lr=0.05).double() #создаём нейронную сеть с перечисленными параметрами\n",
    "model.train_model(5, 0) #тренируем её в течение %e% эпох \n",
    "print(\"Число неправильно классифицированных примеров = {}\".format(evaluate_model(model)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
